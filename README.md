# Adaptive Vector-Quantized Stochastic Gradient Descent
Methods for distributed optimization and training of Neural Networks using Gradient sampling from a convex set. 

We conduct a study on novel projection methods for sparse sampling of gradients in distributed training settings to avoid bottlenecks associated with network communication of gradients. These methods allow us to only propogate gradients from a dynamically updated discrete set, significantly reducing communication requirements in large-scale training. 
